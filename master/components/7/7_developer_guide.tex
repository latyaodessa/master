\section{Developer Guide}\label{Developer Guide}

\subsection{System configuration}\label{System configuration}

\subsubsection{Setting up the database}\label{database setup}

N2Sky uses non-relational database MongoDB, which is running in Docker container. 

First of all the developer has to create  the mongo database container:

 \begin{lstlisting}
	docker run -d -p 27017:27017 --name database \
	 -v ~/dataMongo:/data/db mongo
\end{lstlisting}

It will start the mongo database Docker container on port 27017. 
It is possible to remap ports with the flag "-p".

The second step is to configure N2Sky database:

\emph{Login into container:}  
 \begin{lstlisting}
docker exec -it database mongo
\end{lstlisting}

\emph{Create database superuser:}
 \begin{lstlisting}
 
db.createUser(
	{ user: 'admin', pwd: 'password',
	 roles: [
	 	{ role: "userAdminAnyDatabase", db: "admin" }
			 ] });

\end{lstlisting}



\emph{Create database:}
 \begin{lstlisting}
use n2sky;
\end{lstlisting}


\emph{Create database n2sky user:}
 \begin{lstlisting}
 
db.createUser({user:"n2sky", pwd:"password", roles:['dbOwner']})

\end{lstlisting}

\emph{Create system administrator:}
 \begin{lstlisting}
 
db.users.insert({
   "name":"admin",
   "password":"password",
   "email":"admin@test.com",
   "type":"admin",
   "active":true
});

\end{lstlisting}


\emph{For production version it is recommended to enable authentication:}
 \begin{lstlisting}
 
 	docker rm -f database
 
	docker run -d -p 27017:27017 --name database \
	 -v ~/dataMongo:/data/db mongo mongod --auth

\end{lstlisting}



\subsubsection{Setting up the Cloud Web Service}\label{cloud setup}

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/latyaodessa/n2sky-services.git 
\end{lstlisting}

\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd n2sky-services/services
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}
FROM node:7
WORKDIR /app
COPY package.json .
RUN npm install
RUN npm install -g forever
COPY . /app
CMD forever -c 'node --harmony'  server.js
EXPOSE 8080
\end{lstlisting}

Command: 
 \begin{lstlisting}
docker build -t cloud:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9595:8080 --name cloud cloud:1
\end{lstlisting}


Important to check the host and the port of the database. 
If something goes wrong and the port or the host is changed in n2sky project developer can edit it.
The rest api endpoint host located in \emph{service/config/database.js}

\subsubsection{Setting up the Model Repository Service}\label{model setup}

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/CN2Sky/modelRepository.git
\end{lstlisting}

\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd modelRepository
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}
FROM node:7
WORKDIR /app
COPY package.json .
RUN npm install
RUN npm install -g forever
COPY . /app
CMD forever -c 'node --harmony'  server.js
EXPOSE 8080
\end{lstlisting}

Command: 

 \begin{lstlisting}
docker build -t model:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9092:8080 --name model model:1
\end{lstlisting}



\subsubsection{Setting up the User Management Service}\label{user setup}

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/CN2Sky/user-management.git
\end{lstlisting}

\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd user-management
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}
FROM node:7
WORKDIR /app
COPY package.json .
RUN npm install
RUN npm install -g forever
COPY . /app
CMD forever -c 'node --harmony'  server.js
EXPOSE 8080
\end{lstlisting}

Command: 

 \begin{lstlisting}
docker build -t user:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9091:8080 --name user user:1
\end{lstlisting}



\subsubsection{Setting up the N2Sky frontend}\label{frontend setup}

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/latyaodessa/n2sky-services.git 
\end{lstlisting}

\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd n2sky-services/frontend
\end{lstlisting}

\emph{Build the Docker image}
Dockerfile:
 \begin{lstlisting}
FROM node:7
WORKDIR /app
COPY package.json .
RUN npm install
COPY . /app
CMD npm run dev
EXPOSE 9593
\end{lstlisting}

Command:
 \begin{lstlisting}
docker build -t frontend:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9593:9593 --name frontend frontend:1
\end{lstlisting}





\subsubsection{Setting up the Backpropogation Neural Network}\label{backprop setup}

This neural network made for demonstration purposes. 

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/CN2Sky/backprop.git
\end{lstlisting}

\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd backprop
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}
FROM debian:8

MAINTAINER Kamil Kwiek <kamil.kwiek@continuum.io>

ENV LANG=C.UTF-8 LC_ALL=C.UTF-8

RUN apt-get update --fix-missing && apt-get install -y wget bzip2 ca-certificates \
    libglib2.0-0 libxext6 libsm6 libxrender1 \
    git mercurial subversion

RUN echo 'export PATH=/opt/conda/bin:$PATH' > /etc/profile.d/conda.sh && \
    wget --quiet https://repo.continuum.io/archive/Anaconda2-5.0.1-Linux-x86_64.sh -O \
     ~/anaconda.sh && \
    /bin/bash ~/anaconda.sh -b -p /opt/conda && \
    rm ~/anaconda.sh

RUN apt-get install -y curl grep sed dpkg && \
    TINI_VERSION=`curl https://github.com/krallin/tini/releases/latest | \
    grep -o "/v.*\"" | sed 's:^..\(.*\).$:\1:'` && \
    curl -L "https://github.com/krallin/tini/releases/download/v \
    ${TINI_VERSION}/tini_${TINI_VERSION}.deb" > tini.deb && \
    dpkg -i tini.deb && \
    rm tini.deb && \
    apt-get clean

ENV PATH /opt/conda/bin:$PATH

# Jupyter has issues with being run directly: https://github.com/ipython/ipython/issues/7062
COPY backprop /root/

# Expose Ports for TensorBoard (6006), Ipython (8888)
EXPOSE 6006 8888 5000

WORKDIR "/root"

RUN pip install keras
RUN pip install tensorflow
RUN pip install h5json

CMD ["python", "server.py"]
\end{lstlisting}

Command: 

 \begin{lstlisting}
docker build -t backprop:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 6006:6006 -p 8888:8888 -p 5000:5000 --name backprop backprop:1
\end{lstlisting}







\subsubsection{Setting up the Monitoring System}\label{Monitoring System setup}

To create custom image (OS: Ubuntu 16.04 Cloud version) with monitoring setup following steps has to be completed:

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/latyaodessa/n2sky-services.git 
\end{lstlisting}


\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd n2sky-services/monitoring/prometheus
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}

FROM prom/prometheus:v2.0.0-beta.2
COPY alert.rules /etc/prometheus/alert.rules
COPY test.rules /etc/prometheus/test.rules
COPY node.rules /etc/prometheus/node.rules
COPY ./prometheus.yml /etc/prometheus/prometheus.yml

\end{lstlisting}

Command: 

 \begin{lstlisting}
docker build -t prometheus:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9090:9090 prometheus:1
\end{lstlisting}


In case if developer want to see full stack he needs to deploy node\_exporter:

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/latyaodessa/n2sky-services.git 
\end{lstlisting}


\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd n2sky-services/monitoring/node_exporter 
\end{lstlisting}

\emph{Run the service Docker container and start Monitoring}
 \begin{lstlisting}
docker build -t node_exporter . && \
docker run -d -p 9100:9100 node_exporter && \
cd ../prometheus && \
nohup ./prometheus > /dev/null 2>&1 &
\end{lstlisting}



 In the global configuration is possible to setup scare interval and evaluation interval.
global:
 
 \begin{lstlisting}
  scrape_interval:     15s 
  evaluation_interval: 15s 
\end{lstlisting}

Prometheus has to reference on Alert Manager, where messages will be published. 
 \begin{lstlisting}
alerting:
  alertmanagers:
  - static_configs:
    - targets:
       - localhost:9093
\end{lstlisting}

Every machine where Prometheus is installed can has its own alerting rules. In general alerting rules are located in the root folder of Prometheus.

 \begin{lstlisting}
rule_files:
   - "alert.rules"
   - "node.rules"
   - "test.rules"
\end{lstlisting}

Since there is a need to get more specific data, in N2Sky was decided to user Node Exporter Module. The reference on this module has to be added into configuration

 \begin{lstlisting}
- job_name: 'node'
    scrape_interval: 5s
    target_groups:
-	targets: ['localhost:9100']
\end{lstlisting}

Node Exporter Module has no configuration file. Prometheus listen the modules and scrap the data with a defined interval.

For deploying alert manager Docker containers technology is used.

\subsubsection{Setting up Alert Management System}\label{Setting up Alert Management System}

All configuration of alert manager are written in YAML file. 
On the beginning SMTP email sender should be configured. This would be used to sending notifications.

 \begin{lstlisting}
global:
  smtp_smarthost: 'localhost:25'
  smtp_from: 'alertmanager@example.org'
  smtp_auth_username: 'alertmanager'
  smtp_auth_password: 'password'
\end{lstlisting}

It is possible to define multiple Email templates and configure which template need to be loaded on which severe level. In configuration the path to templates need to be defined. 

 \begin{lstlisting}
templates: 
-	'/etc/alertmanager/template/*.tmpl'
\end{lstlisting}

When alerts are consumed they need to be converted using Email template and fired to the particular route. Every route has a receiver. 

 \begin{lstlisting}
route:
group_by: ['alertname', 'cluster', 'service']
group_wait: 30s
group_interval: 5m
repeat_interval: 3h 
receiver: team-X-mails
\end{lstlisting}

\begin{description}
\item[group\_by] Group by label. This way ensures that multiple alerts from difference cluster can be received
\item[group\_wait] Ensures that multiple alerts can be fired shortly after particular group is received.
\item[group\_interval] Interval between alert batches.
\item[Receiver] Unique name of receiver which is defined in configuration. 
\end{description}

Receiver it is a group of matching by regular expression events.

 \begin{lstlisting}
  routes:
- match_re:
      service: ^(foo1|foo2|baz)
    receiver: team-X-mails
\end{lstlisting}

Receiver can be defined by user configuration, it is an email where is alert notification will be send.

 \begin{lstlisting}
receivers:
- name: 'team-X-mails'
  email_configs:
  - to: 'team-X+alerts@example.org'
\end{lstlisting}

\paragraph{How to write alerting rules} 

The alerting rules are supporting simple query language, which looks very similar to Sequel Query Language.  
There is multiple possibilities how work with a alerting rules. The query language allows to use an expression and as a result to check an attribute of time series. 

 \begin{lstlisting}
ALERT HighLatency
 IF api_http_request_latencies_second{quantile="0.7"} > 1 
FOR 5m 
LABELS { severity="critical"} 
ANNOTATIONS {   summary = "High latency detected ",   description = "over limit? } 
\end{lstlisting}

Following notations should be considered be creation of alerting rules:
\begin{itemize}
\item All queries staring with "ALERT" namespace. After it follows name of alert in this case it is "HighLatency".  
\item "IF" is a condition "api\_http\_request\_latencies\_second", which based on Prometheus Tool expression.  Set of time series with this expression has one parameter it is "quantile". Reading condition as a whole can be translated in a human language like this: "Send a alert if latency request per second bigger then 0.7". 
\item "FOR" it is period of time how often this condition should be checked. 
\item "LABELS" shows a severity level. There are 3 types of severity:
\begin{itemize} 
        \item Critical
        \item Warning
        \item Page
     \end{itemize}
\item Every severity level can be defined on developer needs.
\item "ANNOTATIONS" shows a readable for human comments. There are two sub sections: summary, which shows a short description of the event and description where detailed information about deviation can be written
\end{itemize}

For deploying alert manager Docker containers technology is used.

\emph{Clone the project from the repository}
 \begin{lstlisting}
git clone https://github.com/latyaodessa/n2sky-services.git 
\end{lstlisting}


\emph{Go to service Dockerfile}
 \begin{lstlisting}
cd n2sky-services/monitoring/alertmanager
\end{lstlisting}

\emph{Build the Docker image}

Dockerfile:
 \begin{lstlisting}

FROM prom/alertmanager:v0.10.0
COPY alertmanager.yml /etc/alertmanager/config.yml
ENTRYPOINT [ "/bin/alertmanager" ]
CMD        [ "-config.file=/etc/alertmanager/config.yml", \
             "-storage.path=/alertmanager" ]

\end{lstlisting}

Command: 

 \begin{lstlisting}
docker build -t alertmanager:1 .
\end{lstlisting}


\emph{Run the service Docker container}
 \begin{lstlisting}
docker run -d -p 9093:9093 alertmanager:1
\end{lstlisting}



\subsection{Continuous integration}\label{Continuous integration}

\subsubsection{Create custom image with the monitoring system}\label{Custom image with the monitoring system}

 \begin{lstlisting}

sudo apt-get update && \
sudo apt-get install --assume-yes apt-transport-https ca-certificates \
curl gnupg2 software-properties-common && \
curl -fsSL https://download.docker.com/linux/$(. /etc/os-release; echo "$ID")/gpg \
 | sudo apt-key add - && \
sudo add-apt-repository \
   "deb [arch=amd64] https://download.docker.com/linux/$(. /etc/os-release; echo "$ID") \
   $(lsb_release -cs) \
   stable" && \
sudo apt-get update && \
sudo apt-get install --assume-yes docker-ce && \
sudo usermod -a -G docker $USER && \
sudo bash && \
git clone https://github.com/latyaodessa/n2sky-services.git && \
cd n2sky-services/monitoring/node_exporter && \
docker build -t node_exporter . && \
docker run -d -p 9100:9100 node_exporter && \
cd ../prometheus && \
nohup ./prometheus > /dev/null 2>&1 &

\end{lstlisting}


\subsection{API Documentation}\label{API Documentation}
\subsubsection{N2Sky Monitoring System API Documentation}\label{N2Sky Monitoring System API Documentation}

